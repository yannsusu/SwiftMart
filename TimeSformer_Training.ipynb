{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install simplejson\n",
    "# !pip install fvcore\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from timesformer.models.vit import TimeSformer\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.reset_peak_memory_stats() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'take': 0, 'put': 1, 'drop': 2, 'pick': 3}\n"
     ]
    }
   ],
   "source": [
    "with open(\"trainning data\\labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 215 个训练视频\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"trainning data\"\n",
    "\n",
    "video_paths, video_labels = [], []\n",
    "\n",
    "for action, label in labels.items():\n",
    "    action_dir = os.path.join(dataset_dir, action)\n",
    "    if not os.path.exists(action_dir):\n",
    "        continue  # 跳过不存在的目录\n",
    "\n",
    "    for video_file in os.listdir(action_dir):\n",
    "        if video_file.endswith(\".mp4\"):\n",
    "            video_paths.append(os.path.join(action_dir, video_file))\n",
    "            video_labels.append(label)\n",
    "\n",
    "print(f\"找到 {len(video_paths)} 个训练视频\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(prev_frame, next_frame):\n",
    "    \"\"\"Compute dense optical flow between two frames using Farneback method.\"\"\"\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, \n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Normalize flow to range [0, 255]\n",
    "    flow_x = cv2.normalize(flow[..., 0], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    flow_y = cv2.normalize(flow[..., 1], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Stack as 3-channel (third channel can be zero)\n",
    "    flow_rgb = np.stack([flow_x, flow_y, np.zeros_like(flow_x)], axis=-1)\n",
    "    \n",
    "    return flow_rgb.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(video_path, num_frames=45, resize=(224, 224), mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    prev_frame=None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, resize)  # 统一尺寸\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # OpenCV 默认是 BGR，需要转换\n",
    "        \n",
    "        if prev_frame is not None:\n",
    "            prev_frame = cv2.resize(prev_frame, resize)\n",
    "            flow_frame=compute_optical_flow(prev_frame,frame)\n",
    "            frames.append(flow_frame)\n",
    "\n",
    "        prev_frame=frame\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    frame_count = len(frames)\n",
    "\n",
    "    if frame_count < num_frames:\n",
    "        last_frame = frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        frames.extend([last_frame] * (num_frames - frame_count))\n",
    "    else:\n",
    "        frames = frames[:num_frames]\n",
    "\n",
    "\n",
    "    #convert to tensor cuz NN onlu process numerical data\n",
    "    #/255 normalization to [0,1]\n",
    "    frames=np.array(frames,dtype=np.float32)/255.0\n",
    "    \n",
    "    video_tensor=torch.tensor(frames,dtype=torch.float32)\n",
    "    video_tensor = video_tensor.permute(3, 0, 1, 2)  # (H, W, T, C) → (C, T, H, W)\n",
    "\n",
    "    normalize=transforms.Normalize(mean=mean,std=std)\n",
    "    \n",
    "    # Apply normalization on each channel\n",
    "    for t in range(video_tensor.shape[1]):  # Loop over frames\n",
    "        video_tensor[:, t, :, :] = normalize(video_tensor[:, t, :, :])\n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 45, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "video_tensor = load_video(video_paths[0]) \n",
    "print(video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensors = []\n",
    "for path in video_paths:\n",
    "    video_tensor = load_video(path, num_frames = 45, resize = (224,224))\n",
    "    video_tensors.append(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 215, training dataset size:172,Tensor Shape: torch.Size([215, 3, 45, 224, 224]), 标签: torch.Size([215])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, TensorDataset\n",
    "X = torch.stack(video_tensors).to(device)\n",
    "y = torch.tensor(video_labels, dtype=torch.long).to(device)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size=int(0.8*len(dataset))\n",
    "val_size=len(dataset)-train_size\n",
    "\n",
    "train_dataset,val_dataset=random_split(dataset,[train_size,val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}, training dataset size:{len(train_dataset)},Tensor Shape: {X.shape}, 标签: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with pretrained timesformer model\n",
    "use Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSformer( img_size=224,  # Define image size\n",
    "                    patch_size=16,\n",
    "                    num_classes=4,  # Output for 4 classes\n",
    "                    num_frames=45\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoEMLP(nn.Module):\n",
    "    def __init__(self, in_features=768, hidden_features=3072, num_experts=8, top_k=2):\n",
    "        super(MoEMLP, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k  # Number of experts to activate per input\n",
    "\n",
    "        # Experts (each expert has its own MLP)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features, hidden_features),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_features, in_features)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Gating network (decides which experts to use)\n",
    "        self.gate = nn.Linear(in_features, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "\n",
    "        # Compute gating scores and select top-k experts\n",
    "        gate_logits = self.gate(x)  # (batch, seq_len, num_experts)\n",
    "        top_k_values, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1)  # Get top-k expert indices\n",
    "\n",
    "        # Normalize gate weights using softmax\n",
    "        gate_scores = F.softmax(top_k_values, dim=-1)\n",
    "\n",
    "        # Compute expert outputs\n",
    "        outputs = torch.zeros_like(x)\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = top_k_indices[..., i]\n",
    "            expert_idx = expert_idx.view(-1) \n",
    "            expert_out = torch.stack([self.experts[idx](x[b]) for b, idx in zip(range(x.size(0)),expert_idx)])\n",
    "            outputs += gate_scores[..., i, None] * expert_out  # Weighted sum of experts\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.cls_token', 'model.pos_embed', 'model.patch_embed.proj.weight', 'model.patch_embed.proj.bias', 'model.blocks.0.norm1.weight', 'model.blocks.0.norm1.bias', 'model.blocks.0.attn.qkv.weight', 'model.blocks.0.attn.qkv.bias', 'model.blocks.0.attn.proj.weight', 'model.blocks.0.attn.proj.bias', 'model.blocks.0.temporal_norm1.weight', 'model.blocks.0.temporal_norm1.bias', 'model.blocks.0.temporal_attn.qkv.weight', 'model.blocks.0.temporal_attn.qkv.bias', 'model.blocks.0.temporal_attn.proj.weight', 'model.blocks.0.temporal_attn.proj.bias', 'model.blocks.0.temporal_fc.weight', 'model.blocks.0.temporal_fc.bias', 'model.blocks.0.norm2.weight', 'model.blocks.0.norm2.bias', 'model.blocks.0.mlp.fc1.weight', 'model.blocks.0.mlp.fc1.bias', 'model.blocks.0.mlp.fc2.weight', 'model.blocks.0.mlp.fc2.bias', 'model.blocks.1.norm1.weight', 'model.blocks.1.norm1.bias', 'model.blocks.1.attn.qkv.weight', 'model.blocks.1.attn.qkv.bias', 'model.blocks.1.attn.proj.weight', 'model.blocks.1.attn.proj.bias', 'model.blocks.1.temporal_norm1.weight', 'model.blocks.1.temporal_norm1.bias', 'model.blocks.1.temporal_attn.qkv.weight', 'model.blocks.1.temporal_attn.qkv.bias', 'model.blocks.1.temporal_attn.proj.weight', 'model.blocks.1.temporal_attn.proj.bias', 'model.blocks.1.temporal_fc.weight', 'model.blocks.1.temporal_fc.bias', 'model.blocks.1.norm2.weight', 'model.blocks.1.norm2.bias', 'model.blocks.1.mlp.fc1.weight', 'model.blocks.1.mlp.fc1.bias', 'model.blocks.1.mlp.fc2.weight', 'model.blocks.1.mlp.fc2.bias', 'model.blocks.2.norm1.weight', 'model.blocks.2.norm1.bias', 'model.blocks.2.attn.qkv.weight', 'model.blocks.2.attn.qkv.bias', 'model.blocks.2.attn.proj.weight', 'model.blocks.2.attn.proj.bias', 'model.blocks.2.temporal_norm1.weight', 'model.blocks.2.temporal_norm1.bias', 'model.blocks.2.temporal_attn.qkv.weight', 'model.blocks.2.temporal_attn.qkv.bias', 'model.blocks.2.temporal_attn.proj.weight', 'model.blocks.2.temporal_attn.proj.bias', 'model.blocks.2.temporal_fc.weight', 'model.blocks.2.temporal_fc.bias', 'model.blocks.2.norm2.weight', 'model.blocks.2.norm2.bias', 'model.blocks.2.mlp.fc1.weight', 'model.blocks.2.mlp.fc1.bias', 'model.blocks.2.mlp.fc2.weight', 'model.blocks.2.mlp.fc2.bias', 'model.blocks.3.norm1.weight', 'model.blocks.3.norm1.bias', 'model.blocks.3.attn.qkv.weight', 'model.blocks.3.attn.qkv.bias', 'model.blocks.3.attn.proj.weight', 'model.blocks.3.attn.proj.bias', 'model.blocks.3.temporal_norm1.weight', 'model.blocks.3.temporal_norm1.bias', 'model.blocks.3.temporal_attn.qkv.weight', 'model.blocks.3.temporal_attn.qkv.bias', 'model.blocks.3.temporal_attn.proj.weight', 'model.blocks.3.temporal_attn.proj.bias', 'model.blocks.3.temporal_fc.weight', 'model.blocks.3.temporal_fc.bias', 'model.blocks.3.norm2.weight', 'model.blocks.3.norm2.bias', 'model.blocks.3.mlp.fc1.weight', 'model.blocks.3.mlp.fc1.bias', 'model.blocks.3.mlp.fc2.weight', 'model.blocks.3.mlp.fc2.bias', 'model.blocks.4.norm1.weight', 'model.blocks.4.norm1.bias', 'model.blocks.4.attn.qkv.weight', 'model.blocks.4.attn.qkv.bias', 'model.blocks.4.attn.proj.weight', 'model.blocks.4.attn.proj.bias', 'model.blocks.4.temporal_norm1.weight', 'model.blocks.4.temporal_norm1.bias', 'model.blocks.4.temporal_attn.qkv.weight', 'model.blocks.4.temporal_attn.qkv.bias', 'model.blocks.4.temporal_attn.proj.weight', 'model.blocks.4.temporal_attn.proj.bias', 'model.blocks.4.temporal_fc.weight', 'model.blocks.4.temporal_fc.bias', 'model.blocks.4.norm2.weight', 'model.blocks.4.norm2.bias', 'model.blocks.4.mlp.fc1.weight', 'model.blocks.4.mlp.fc1.bias', 'model.blocks.4.mlp.fc2.weight', 'model.blocks.4.mlp.fc2.bias', 'model.blocks.5.norm1.weight', 'model.blocks.5.norm1.bias', 'model.blocks.5.attn.qkv.weight', 'model.blocks.5.attn.qkv.bias', 'model.blocks.5.attn.proj.weight', 'model.blocks.5.attn.proj.bias', 'model.blocks.5.temporal_norm1.weight', 'model.blocks.5.temporal_norm1.bias', 'model.blocks.5.temporal_attn.qkv.weight', 'model.blocks.5.temporal_attn.qkv.bias', 'model.blocks.5.temporal_attn.proj.weight', 'model.blocks.5.temporal_attn.proj.bias', 'model.blocks.5.temporal_fc.weight', 'model.blocks.5.temporal_fc.bias', 'model.blocks.5.norm2.weight', 'model.blocks.5.norm2.bias', 'model.blocks.5.mlp.fc1.weight', 'model.blocks.5.mlp.fc1.bias', 'model.blocks.5.mlp.fc2.weight', 'model.blocks.5.mlp.fc2.bias', 'model.blocks.6.norm1.weight', 'model.blocks.6.norm1.bias', 'model.blocks.6.attn.qkv.weight', 'model.blocks.6.attn.qkv.bias', 'model.blocks.6.attn.proj.weight', 'model.blocks.6.attn.proj.bias', 'model.blocks.6.temporal_norm1.weight', 'model.blocks.6.temporal_norm1.bias', 'model.blocks.6.temporal_attn.qkv.weight', 'model.blocks.6.temporal_attn.qkv.bias', 'model.blocks.6.temporal_attn.proj.weight', 'model.blocks.6.temporal_attn.proj.bias', 'model.blocks.6.temporal_fc.weight', 'model.blocks.6.temporal_fc.bias', 'model.blocks.6.norm2.weight', 'model.blocks.6.norm2.bias', 'model.blocks.6.mlp.fc1.weight', 'model.blocks.6.mlp.fc1.bias', 'model.blocks.6.mlp.fc2.weight', 'model.blocks.6.mlp.fc2.bias', 'model.blocks.7.norm1.weight', 'model.blocks.7.norm1.bias', 'model.blocks.7.attn.qkv.weight', 'model.blocks.7.attn.qkv.bias', 'model.blocks.7.attn.proj.weight', 'model.blocks.7.attn.proj.bias', 'model.blocks.7.temporal_norm1.weight', 'model.blocks.7.temporal_norm1.bias', 'model.blocks.7.temporal_attn.qkv.weight', 'model.blocks.7.temporal_attn.qkv.bias', 'model.blocks.7.temporal_attn.proj.weight', 'model.blocks.7.temporal_attn.proj.bias', 'model.blocks.7.temporal_fc.weight', 'model.blocks.7.temporal_fc.bias', 'model.blocks.7.norm2.weight', 'model.blocks.7.norm2.bias', 'model.blocks.7.mlp.fc1.weight', 'model.blocks.7.mlp.fc1.bias', 'model.blocks.7.mlp.fc2.weight', 'model.blocks.7.mlp.fc2.bias', 'model.blocks.8.norm1.weight', 'model.blocks.8.norm1.bias', 'model.blocks.8.attn.qkv.weight', 'model.blocks.8.attn.qkv.bias', 'model.blocks.8.attn.proj.weight', 'model.blocks.8.attn.proj.bias', 'model.blocks.8.temporal_norm1.weight', 'model.blocks.8.temporal_norm1.bias', 'model.blocks.8.temporal_attn.qkv.weight', 'model.blocks.8.temporal_attn.qkv.bias', 'model.blocks.8.temporal_attn.proj.weight', 'model.blocks.8.temporal_attn.proj.bias', 'model.blocks.8.temporal_fc.weight', 'model.blocks.8.temporal_fc.bias', 'model.blocks.8.norm2.weight', 'model.blocks.8.norm2.bias', 'model.blocks.8.mlp.fc1.weight', 'model.blocks.8.mlp.fc1.bias', 'model.blocks.8.mlp.fc2.weight', 'model.blocks.8.mlp.fc2.bias', 'model.blocks.9.norm1.weight', 'model.blocks.9.norm1.bias', 'model.blocks.9.attn.qkv.weight', 'model.blocks.9.attn.qkv.bias', 'model.blocks.9.attn.proj.weight', 'model.blocks.9.attn.proj.bias', 'model.blocks.9.temporal_norm1.weight', 'model.blocks.9.temporal_norm1.bias', 'model.blocks.9.temporal_attn.qkv.weight', 'model.blocks.9.temporal_attn.qkv.bias', 'model.blocks.9.temporal_attn.proj.weight', 'model.blocks.9.temporal_attn.proj.bias', 'model.blocks.9.temporal_fc.weight', 'model.blocks.9.temporal_fc.bias', 'model.blocks.9.norm2.weight', 'model.blocks.9.norm2.bias', 'model.blocks.9.mlp.fc1.weight', 'model.blocks.9.mlp.fc1.bias', 'model.blocks.9.mlp.fc2.weight', 'model.blocks.9.mlp.fc2.bias', 'model.blocks.10.norm1.weight', 'model.blocks.10.norm1.bias', 'model.blocks.10.attn.qkv.weight', 'model.blocks.10.attn.qkv.bias', 'model.blocks.10.attn.proj.weight', 'model.blocks.10.attn.proj.bias', 'model.blocks.10.temporal_norm1.weight', 'model.blocks.10.temporal_norm1.bias', 'model.blocks.10.temporal_attn.qkv.weight', 'model.blocks.10.temporal_attn.qkv.bias', 'model.blocks.10.temporal_attn.proj.weight', 'model.blocks.10.temporal_attn.proj.bias', 'model.blocks.10.temporal_fc.weight', 'model.blocks.10.temporal_fc.bias', 'model.blocks.10.norm2.weight', 'model.blocks.10.norm2.bias', 'model.blocks.10.mlp.fc1.weight', 'model.blocks.10.mlp.fc1.bias', 'model.blocks.10.mlp.fc2.weight', 'model.blocks.10.mlp.fc2.bias', 'model.blocks.11.norm1.weight', 'model.blocks.11.norm1.bias', 'model.blocks.11.attn.qkv.weight', 'model.blocks.11.attn.qkv.bias', 'model.blocks.11.attn.proj.weight', 'model.blocks.11.attn.proj.bias', 'model.blocks.11.temporal_norm1.weight', 'model.blocks.11.temporal_norm1.bias', 'model.blocks.11.temporal_attn.qkv.weight', 'model.blocks.11.temporal_attn.qkv.bias', 'model.blocks.11.temporal_attn.proj.weight', 'model.blocks.11.temporal_attn.proj.bias', 'model.blocks.11.temporal_fc.weight', 'model.blocks.11.temporal_fc.bias', 'model.blocks.11.norm2.weight', 'model.blocks.11.norm2.bias', 'model.blocks.11.mlp.fc1.weight', 'model.blocks.11.mlp.fc1.bias', 'model.blocks.11.mlp.fc2.weight', 'model.blocks.11.mlp.fc2.bias', 'model.norm.weight', 'model.norm.bias'])\n",
      "Removed keys: ['model.blocks.0.mlp.fc1.weight', 'model.blocks.0.mlp.fc1.bias', 'model.blocks.0.mlp.fc2.weight', 'model.blocks.0.mlp.fc2.bias', 'model.blocks.1.mlp.fc1.weight', 'model.blocks.1.mlp.fc1.bias', 'model.blocks.1.mlp.fc2.weight', 'model.blocks.1.mlp.fc2.bias', 'model.blocks.2.mlp.fc1.weight', 'model.blocks.2.mlp.fc1.bias', 'model.blocks.2.mlp.fc2.weight', 'model.blocks.2.mlp.fc2.bias', 'model.blocks.3.mlp.fc1.weight', 'model.blocks.3.mlp.fc1.bias', 'model.blocks.3.mlp.fc2.weight', 'model.blocks.3.mlp.fc2.bias', 'model.blocks.4.mlp.fc1.weight', 'model.blocks.4.mlp.fc1.bias', 'model.blocks.4.mlp.fc2.weight', 'model.blocks.4.mlp.fc2.bias', 'model.blocks.5.mlp.fc1.weight', 'model.blocks.5.mlp.fc1.bias', 'model.blocks.5.mlp.fc2.weight', 'model.blocks.5.mlp.fc2.bias', 'model.blocks.6.mlp.fc1.weight', 'model.blocks.6.mlp.fc1.bias', 'model.blocks.6.mlp.fc2.weight', 'model.blocks.6.mlp.fc2.bias', 'model.blocks.7.mlp.fc1.weight', 'model.blocks.7.mlp.fc1.bias', 'model.blocks.7.mlp.fc2.weight', 'model.blocks.7.mlp.fc2.bias', 'model.blocks.8.mlp.fc1.weight', 'model.blocks.8.mlp.fc1.bias', 'model.blocks.8.mlp.fc2.weight', 'model.blocks.8.mlp.fc2.bias', 'model.blocks.9.mlp.fc1.weight', 'model.blocks.9.mlp.fc1.bias', 'model.blocks.9.mlp.fc2.weight', 'model.blocks.9.mlp.fc2.bias', 'model.blocks.10.mlp.fc1.weight', 'model.blocks.10.mlp.fc1.bias', 'model.blocks.10.mlp.fc2.weight', 'model.blocks.10.mlp.fc2.bias', 'model.blocks.11.mlp.fc1.weight', 'model.blocks.11.mlp.fc1.bias', 'model.blocks.11.mlp.fc2.weight', 'model.blocks.11.mlp.fc2.bias']\n",
      "TimeSformer(\n",
      "  (model): VisionTransformer(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (time_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (temporal_attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (temporal_attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"TimeSformer_divST_8x32_224_K400.pyth\", map_location=\"cuda\")\n",
    "pretrained_dict = checkpoint['model_state']\n",
    "# Resize positional embedding to match new num_frames\n",
    "pretrained_time_embed = checkpoint['model_state'][\"model.time_embed\"]  # Shape: [1, 8, 768]\n",
    "\n",
    "# Remove the `head` and `time_embed` layers from the checkpoint (to avoid loading mismatched layers)\n",
    "pretrained_dict.pop('model.time_embed', None)\n",
    "pretrained_dict.pop('model.head.weight', None)\n",
    "pretrained_dict.pop('model.head.bias', None)\n",
    "\n",
    "print(pretrained_dict.keys())\n",
    "pretrained_dict={k: v for k, v in pretrained_dict.items() if \"mlp.fc1\" not in k or \"mlp.fc2\" not in k}\n",
    "\n",
    "# Check removed keys\n",
    "print(\"Removed keys:\", [k for k in pretrained_dict if \"mlp.fc1\" in k or \"mlp.fc2\" in k])\n",
    "\n",
    "\n",
    "# Load the rest of the checkpoint into the model (this will load all layers except the modified ones)\n",
    "model.load_state_dict(pretrained_dict, strict=False)\n",
    "\n",
    "# Modify classification head for 4 classes\n",
    "model.head = nn.Linear(768, 4)\n",
    "\n",
    "\n",
    "\n",
    "# Permute to [1, 768, 8] for interpolation\n",
    "pretrained_time_embed = pretrained_time_embed.permute(0, 2, 1)\n",
    "new_time_embed = F.interpolate(pretrained_time_embed, size=45, mode=\"linear\", align_corners=False)\n",
    "new_time_embed = new_time_embed.permute(0, 2, 1)\n",
    "\n",
    "# Set the modified time_embed layer\n",
    "model.time_embed = nn.Parameter(new_time_embed)\n",
    "\n",
    "# Print the model architecture to verify changes\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model.model.blocks:\n",
    "    block.mlp = MoEMLP(in_features=768, hidden_features=3072, num_experts=4, top_k=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in model.model.blocks:\n",
    "    for param in block.mlp.parameters():\n",
    "        if len(param.shape) == 2 and param.requires_grad:\n",
    "            nn.init.xavier_uniform_(param)  # Xavier init for stability\n",
    "        elif len(param.shape)==1 and param.requires_grad:\n",
    "            nn.init.zeros_(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSformer(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "    \n",
    "#     progress_bar=tqdm(train_loader,desc=f'Epoch{epoch+1}')\n",
    "#     for batch_videos, batch_labels in progress_bar:\n",
    "#         batch_videos, batch_labels = batch_videos.to(device), batch_labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_videos)\n",
    "#         loss = loss_fn(outputs, batch_labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         #total_loss += loss.item()\n",
    "#         progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "#     #print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# #print(\"训练完成！\")\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# val_loss=0.0\n",
    "# correct=0\n",
    "# total=0\n",
    "# predictions = []\n",
    "# ground_truth = []\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in val_loader:\n",
    "#         images, labels=images.to(device),labels.to(device)\n",
    "\n",
    "#         outputs=model(images)\n",
    "#         loss=loss_fn(outputs,labels)\n",
    "#         val_loss+=loss.item()\n",
    "\n",
    "#         _, predicted=torch.max(outputs,1)\n",
    "#         total+=labels.size(0)\n",
    "#         correct+=(predicted==labels).sum().item()\n",
    "\n",
    "#         progress_bar.set_postfix({'val_loss':loss.item()})\n",
    "\n",
    "\n",
    "# # Average loss and accuracy\n",
    "# val_loss /= len(val_loader)\n",
    "# accuracy = 100 * correct / total\n",
    "# print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch1: 100%|██████████| 86/86 [1:48:44<00:00, 75.87s/it, loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 123.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch2: 100%|██████████| 86/86 [1:51:26<00:00, 77.76s/it, loss=1.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 108.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch3: 100%|██████████| 86/86 [1:50:22<00:00, 77.00s/it, loss=1.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 108.5699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch4: 100%|██████████| 86/86 [1:51:01<00:00, 77.46s/it, loss=1.81] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 110.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch5: 100%|██████████| 86/86 [1:51:04<00:00, 77.49s/it, loss=1.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 99.1488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch6: 100%|██████████| 86/86 [1:49:24<00:00, 76.34s/it, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 85.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch7: 100%|██████████| 86/86 [1:48:57<00:00, 76.02s/it, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 77.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch8: 100%|██████████| 86/86 [1:49:28<00:00, 76.38s/it, loss=1.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 77.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch9: 100%|██████████| 86/86 [1:48:53<00:00, 75.97s/it, loss=0.19]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 64.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch10: 100%|██████████| 86/86 [1:50:37<00:00, 77.18s/it, loss=1.04]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 68.5458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch11: 100%|██████████| 86/86 [1:50:39<00:00, 77.20s/it, loss=0.287] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 56.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch12: 100%|██████████| 86/86 [1:49:57<00:00, 76.72s/it, loss=0.202] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 49.4753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch13: 100%|██████████| 86/86 [1:50:14<00:00, 76.91s/it, loss=0.183]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 37.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch14: 100%|██████████| 86/86 [1:50:14<00:00, 76.91s/it, loss=1.21]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 33.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch15: 100%|██████████| 86/86 [1:48:22<00:00, 75.61s/it, loss=0.0169]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 35.1390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch16: 100%|██████████| 86/86 [1:49:14<00:00, 76.22s/it, loss=0.0505]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 23.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch17: 100%|██████████| 86/86 [1:48:38<00:00, 75.80s/it, loss=0.0681] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 25.4924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch18: 100%|██████████| 86/86 [1:35:36<00:00, 66.71s/it, loss=0.0761]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 18.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch19: 100%|██████████| 86/86 [1:32:41<00:00, 64.67s/it, loss=0.046]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 19.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch20: 100%|██████████| 86/86 [1:32:18<00:00, 64.40s/it, loss=0.0717] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 20.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 43/43 [00:39<00:00,  1.10it/s, val_loss=0.387]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7125, Accuracy: 72.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "accumulation_steps = 4\n",
    "scaler = GradScaler()\n",
    "best_loss=float('inf')\n",
    "patience_count=0\n",
    "patience=5\n",
    "\n",
    "#假如混合精度和梯度累积\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    progress_bar=tqdm(train_loader,desc=f'Epoch{epoch+1}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss=0.0\n",
    "\n",
    "    #需要加一个i来计算参数更新\n",
    "    for i, (batch_videos, batch_labels) in enumerate(progress_bar):\n",
    "        batch_videos, batch_labels = batch_videos.to(device), batch_labels.to(device)\n",
    "        \n",
    "        #用混合进度计算损失，同时进行梯度缩放\n",
    "        with autocast(device_type='cuda',dtype=torch.float16):\n",
    "           outputs = model(batch_videos)\n",
    "           loss = loss_fn(outputs, batch_labels)/ accumulation_steps\n",
    "        \n",
    "        \n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # 每累计4次更新一次参数\n",
    "        if (i + 1) % accumulation_steps == 0 or (i+1==len(train_loader)):\n",
    "           scaler.step(optimizer)\n",
    "           scaler.update()\n",
    "           optimizer.zero_grad()\n",
    "        # 显示时回归原始损失\n",
    "        total_loss+=loss.item()*accumulation_steps\n",
    "        progress_bar.set_postfix({\"loss\": loss.item() * accumulation_steps})\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "    if total_loss<best_loss:\n",
    "        best_loss=total_loss\n",
    "        patience_count=0\n",
    "        torch.save(model.state_dict(),'trained_timeSformer.pth')\n",
    "    else:\n",
    "        patience_count+=1\n",
    "    \n",
    "    if patience_count>=patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('trained_timeSformer.pth'))\n",
    "\n",
    "#validation\n",
    "model.eval()\n",
    "val_loss=0.0\n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "progress_bar = tqdm(val_loader, desc=\"Validation\") \n",
    "with torch.no_grad():\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels=images.to(device),labels.to(device)\n",
    "\n",
    "        outputs=model(images)\n",
    "        loss=loss_fn(outputs,labels)\n",
    "        val_loss+=loss.item()\n",
    "\n",
    "        _, predicted=torch.max(outputs,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({'val_loss':loss.item()})\n",
    "\n",
    "\n",
    "# Average loss and accuracy\n",
    "val_loss /= len(val_loader)\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"full_timeSformer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSformer(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = torch.load('full_timeSformer.pth',weights_only=False)\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSformer(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MoEMLP(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.eval()\n",
    "new_model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset\n",
    "X = torch.stack(video_tensors).to(device)\n",
    "y = torch.tensor(video_labels, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset=TensorDataset(X,y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 0\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 0, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 0, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 0, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 0, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 1, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 1, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 1, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 2, Actual: 2\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 2, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 3, Actual: 3\n",
      "Class 0 Accuracy: 84.81%\n",
      "Class 1 Accuracy: 95.18%\n",
      "Class 2 Accuracy: 83.33%\n",
      "Class 3 Accuracy: 97.14%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_accuracies[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m whole_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "num_classes=4\n",
    "correct = [0]*num_classes\n",
    "total = [0]*num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, labels in test_loader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        outputs = new_model(videos)  # Forward pass\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "\n",
    "        for label,pred in zip(labels,predicted):\n",
    "            total[label.item()] += 1\n",
    "            if label==pred:\n",
    "                correct[label.item()] +=1\n",
    "\n",
    "        print(f\"Predicted: {predicted.item()}, Actual: {labels.item()}\")\n",
    "\n",
    "# Compute accuracy\n",
    "class_accuracies=[100*correct[i]/total[i] if total[i]>0 else 0 for i in range(num_classes)] \n",
    "# Print accuracy for each class\n",
    "for i in range(num_classes):\n",
    "    print(f\"Class {i} Accuracy: {class_accuracies[i]:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
