{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T08:39:20.673016Z",
     "start_time": "2025-04-22T08:39:13.583575Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from timesformer.models.vit import TimeSformer\n",
    "from tqdm import tqdm \n",
    "from torchvision import transforms\n",
    "from models.timersformer_moe import MoEMLP"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timesformer'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m read_video\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TensorDataset, DataLoader\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtimesformer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvit\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TimeSformer\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm \n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m transforms\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'timesformer'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = './output_7/'\n",
    "\n",
    "# Sort files numerically (excluding extensions)\n",
    "files = sorted(os.listdir(folder), key=lambda x: int(os.path.splitext(x)[0]))  # sort by number\n",
    "\n",
    "for idx, filename in enumerate(files, start=1):\n",
    "    # Ensure filename ends with .jpg (or any other image type you need)\n",
    "    src = os.path.join(folder, filename)\n",
    "    dst = os.path.join(folder, f'{idx:06d}.jpg')  # This will pad with zeroes (000001.jpg)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a detect human function\n",
    "def detect_humans():\n",
    "    detections={}\n",
    "    with open(\"./tracking.seq\", \"r\") as f:\n",
    "        for line in f:\n",
    "            frame_id, person_id, x1, y1, x2, y2 = map(int, line.strip().split(\",\"))\n",
    "            if frame_id not in detections:\n",
    "                detections[frame_id] = []\n",
    "            detections[frame_id].append((x1, y1, x2, y2, person_id))\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sliding_window(sequence,window_size=45,stride=22):\n",
    "    num_frames=len(sequence)\n",
    "    windows=[]\n",
    "\n",
    "    for start in range(0, num_frames-window_size+1,stride):\n",
    "        window=sequence[start:start+window_size]\n",
    "        windows.append(window)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(prev_patch, next_patch):\n",
    "    \"\"\"Compute dense optical flow between two frames using Farneback method.\"\"\"\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_patch, next_patch, None, \n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Normalize flow to range [0, 255]\n",
    "    flow_x = cv2.normalize(flow[..., 0], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    flow_y = cv2.normalize(flow[..., 1], None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # # Stack as 3-channel (third channel can be zero)\n",
    "    # flow_rgb = np.stack([flow_x, flow_y, np.zeros_like(flow_x)], axis=-1)\n",
    "    \n",
    "    return np.stack([flow_x,flow_y,np.zeros_like(flow_x)],axis=-1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_frames(frame_folder,timesformer_model,num_frames=45,resize=(224,224), threshold=0.6):\n",
    "#     #detections = detect_humans(video_path)  # Dictionary {frame_idx: [(x1, y1, x2, y2, person_id), ...]}\n",
    "#     detections=detect_humans()\n",
    "\n",
    "#     frame_idx=0\n",
    "#     person_patches={}\n",
    "#     prev_patches={}\n",
    "#     frame_indices=sorted(detections.keys())\n",
    "    \n",
    "#     for frame_idx in frame_indices:\n",
    "#         frame_path=os.path.join(frame_folder,f\"{frame_idx:06}.jpg\")\n",
    "#         frame=cv2.imread(frame_path)\n",
    "\n",
    "#         if frame is None:\n",
    "#             print(f\"Warning:Frame{frame_path} not found.\")\n",
    "#             continue\n",
    "#         for (x1,y1,x2,y2,person_id) in detections[frame_idx]:\n",
    "#             patch=frame[y1:y2,x1:x2]\n",
    "\n",
    "#             if patch.size==0:\n",
    "#                 print(f\"Warning:Empty patch at frame{frame_idx},person {person_id}\")\n",
    "#                 continue\n",
    "\n",
    "#             person_patch=cv2.resize(patch,resize)\n",
    "#             person_patch=cv2.cvtColor(person_patch, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #         if person_id in prev_patches:\n",
    "    #                 prev_patch = prev_patches[person_id]\n",
    "    #                 flow_patch = compute_optical_flow(prev_patch, person_patch)\n",
    "\n",
    "    #                 if person_id not in person_patches:\n",
    "    #                     person_patches[person_id] = []\n",
    "\n",
    "    #                 person_patches[person_id].append(flow_patch)\n",
    "\n",
    "    #         prev_patches[person_id] = person_patch\n",
    "\n",
    "\n",
    "    # # Convert to tensors\n",
    "    # person_tensors = {}\n",
    "    # for person_id, patches in person_patches.items():\n",
    "    #     if len(patches) < num_frames:\n",
    "    #         last_patch = patches[-1] if patches else np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    #         patches.extend([last_patch] * (num_frames - len(patches)))\n",
    "    #     else:\n",
    "    #         patches = patches[:num_frames]\n",
    "\n",
    "    #     # Convert to tensor and normalize\n",
    "    #     patches = np.array(patches, dtype=np.float32) / 255.0\n",
    "    #     video_tensor = torch.tensor(patches, dtype=torch.float32).permute(3, 0, 1, 2)\n",
    "\n",
    "    #     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #     for t in range(video_tensor.shape[1]):\n",
    "    #         video_tensor[:, t, :, :] = normalize(video_tensor[:, t, :, :])\n",
    "\n",
    "    #     person_tensors[person_id] = video_tensor.unsqueeze(0)  # Add batch dim (1, C, T, H, W)\n",
    "\n",
    "    #  # Pass through TimeSformer model\n",
    "    # results = {}\n",
    "    # timesformer_model = torch.load('full_timeSformer.pth',weights_only=False)\n",
    "    # timesformer_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # timesformer_model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for person_id, tensor in person_tensors.items():\n",
    "    #         tensor = tensor.to('cuda')  # Move to GPU if available\n",
    "            \n",
    "    #         outputs = timesformer_model(tensor)  # Forward pass\n",
    "    #         probs = torch.softmax(outputs, dim=1)\n",
    "    #         predicted_label = torch.argmax(probs, dim=1).item()\n",
    "    #         confidence = probs.max().item()\n",
    "\n",
    "    #         if confidence<threshold:\n",
    "    #             predicted_label=-1\n",
    "\n",
    "    #         results[person_id] = {\"action\": predicted_label, \"confidence\": confidence}\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def process_frames(frame_folder, timesformer_model, num_frames=45, resize=(224, 224), threshold=0.6):\n",
    "    detections = detect_humans()  # Should return {frame_idx: [(x1, y1, x2, y2, person_id), ...]}\n",
    "    \n",
    "    frame_indices = sorted(detections.keys())\n",
    "    person_patches = {}\n",
    "    prev_patches = {}\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        frame_path = os.path.join(frame_folder, f\"{frame_idx:06}.jpg\")\n",
    "        frame = cv2.imread(frame_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Frame {frame_path} not found.\")\n",
    "            continue\n",
    "\n",
    "        for (x1, y1, x2, y2, person_id) in detections[frame_idx]:\n",
    "            patch = frame[y1:y2, x1:x2]\n",
    "            if patch.size == 0:\n",
    "                print(f\"Warning: Empty patch at frame {frame_idx}, person {person_id}\")\n",
    "                continue\n",
    "\n",
    "            # Resize and convert to grayscale\n",
    "            patch = cv2.resize(patch, resize)\n",
    "            gray_patch = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Optical flow between current and previous frame\n",
    "            if person_id in prev_patches:\n",
    "                prev_patch = prev_patches[person_id]\n",
    "                flow = compute_optical_flow(prev_patch, gray_patch)  # Output shape (H,W,2) or (H,W,3)\n",
    "\n",
    "                if person_id not in person_patches:\n",
    "                    person_patches[person_id] = []\n",
    "                person_patches[person_id].append(flow)\n",
    "\n",
    "            prev_patches[person_id] = gray_patch\n",
    "\n",
    "    # Padding/Trimming and Tensor conversion\n",
    "    person_tensors = {}\n",
    "    for person_id, patches in person_patches.items():\n",
    "        if len(patches) < num_frames:\n",
    "            last_patch = patches[-1] if patches else np.zeros((resize[0], resize[1], 3), dtype=np.float32)\n",
    "            patches.extend([last_patch] * (num_frames - len(patches)))\n",
    "        else:\n",
    "            patches = patches[:num_frames]\n",
    "\n",
    "        patches = np.array(patches, dtype=np.float32) / 255.0  # Normalize to [0,1]\n",
    "        video_tensor = torch.tensor(patches, dtype=torch.float32).permute(3, 0, 1, 2)  # (C,T,H,W)\n",
    "\n",
    "        # Normalization (RGB optical flow assumed)\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        for t in range(video_tensor.shape[1]):\n",
    "            video_tensor[:, t, :, :] = normalize(video_tensor[:, t, :, :])\n",
    "\n",
    "        person_tensors[person_id] = video_tensor.unsqueeze(0)  # (1,C,T,H,W)\n",
    "\n",
    "    # Load Model & Inference\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    timesformer_model.to(device)\n",
    "    timesformer_model.eval()\n",
    "\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for person_id, tensor in person_tensors.items():\n",
    "            tensor = tensor.to(device)\n",
    "            outputs = timesformer_model(tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_label = torch.argmax(probs, dim=1).item()\n",
    "            confidence = probs.max().item()\n",
    "\n",
    "            if confidence < threshold:\n",
    "                pred_label = -1  # Unknown action\n",
    "\n",
    "            results[person_id] = {\n",
    "                \"action\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(frame_folder, timesformer_model, num_frames=45, resize=(224, 224), threshold=0.6, stride=22):\n",
    "    detections = detect_humans()  # {frame_idx: [(x1, y1, x2, y2, person_id), ...]}\n",
    "\n",
    "    frame_indices = sorted(detections.keys())\n",
    "    person_patches = {}\n",
    "    prev_patches = {}\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        frame_path = os.path.join(frame_folder, f\"{frame_idx:06}.jpg\")\n",
    "        frame = cv2.imread(frame_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Frame {frame_path} not found.\")\n",
    "            continue\n",
    "\n",
    "        for (x1, y1, x2, y2, person_id) in detections[frame_idx]:\n",
    "            patch = frame[y1:y2, x1:x2]\n",
    "            if patch.size == 0:\n",
    "                print(f\"Warning: Empty patch at frame {frame_idx}, person {person_id}\")\n",
    "                continue\n",
    "\n",
    "            person_patch = cv2.resize(patch, resize)\n",
    "            person_patch = cv2.cvtColor(person_patch, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if person_id in prev_patches:\n",
    "                prev_patch = prev_patches[person_id]\n",
    "                flow_patch = compute_optical_flow(prev_patch, person_patch)\n",
    "\n",
    "                if person_id not in person_patches:\n",
    "                    person_patches[person_id] = []\n",
    "\n",
    "                person_patches[person_id].append(flow_patch)\n",
    "\n",
    "            prev_patches[person_id] = person_patch\n",
    "\n",
    "    results = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    timesformer_model.to(device)\n",
    "    timesformer_model.eval()\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for person_id, patches in person_patches.items():\n",
    "            if len(patches) < num_frames:\n",
    "                continue  # Not enough frames to classify\n",
    "\n",
    "            results[person_id] = []\n",
    "\n",
    "            for start in range(0, len(patches) - num_frames + 1, stride):\n",
    "                window = patches[start:start + num_frames]\n",
    "                window = np.array(window, dtype=np.float32) / 255.0  # Normalize\n",
    "                # print(window.shape)\n",
    "\n",
    "                video_tensor = torch.tensor(window) # (T, H, W, 3)\n",
    "                video_tensor = video_tensor.permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "\n",
    "                for t in range(video_tensor.shape[1]):\n",
    "                    video_tensor[:, t, :, :] = normalize(video_tensor[:, t, :, :])\n",
    "\n",
    "                video_tensor = video_tensor.unsqueeze(0).to(device)  # Add batch dim\n",
    "\n",
    "                outputs = timesformer_model(video_tensor)\n",
    "                predicted_label= torch.softmax(outputs, dim=1).argmax(dim=1).item()\n",
    "                confidence = torch.softmax(outputs,dim=1).max().item()\n",
    "\n",
    "                if confidence < threshold:\n",
    "                    predicted_label = -1\n",
    "\n",
    "                results[person_id].append({\n",
    "                    \"action\": predicted_label,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"start_frame\": frame_indices[start],\n",
    "                    \"end_frame\": frame_indices[start + num_frames - 1]\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesformer=torch.load('full_timeSformer.pth',weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results=process_frames(frame_folder=\"./output_7/\",timesformer_model=timesformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [{'action': -1, 'confidence': 0.43191686272621155, 'start_frame': 18, 'end_frame': 62}, {'action': -1, 'confidence': 0.4689951241016388, 'start_frame': 40, 'end_frame': 92}, {'action': 3, 'confidence': 0.6006878614425659, 'start_frame': 62, 'end_frame': 114}, {'action': 3, 'confidence': 0.6938800811767578, 'start_frame': 92, 'end_frame': 136}, {'action': -1, 'confidence': 0.5224723815917969, 'start_frame': 114, 'end_frame': 158}, {'action': 3, 'confidence': 0.9364585280418396, 'start_frame': 136, 'end_frame': 180}, {'action': 3, 'confidence': 0.8982377052307129, 'start_frame': 158, 'end_frame': 202}, {'action': 3, 'confidence': 0.6278750896453857, 'start_frame': 180, 'end_frame': 224}, {'action': 2, 'confidence': 0.8564113974571228, 'start_frame': 202, 'end_frame': 246}, {'action': 3, 'confidence': 0.6683403253555298, 'start_frame': 224, 'end_frame': 268}, {'action': 0, 'confidence': 0.6907159090042114, 'start_frame': 246, 'end_frame': 290}, {'action': -1, 'confidence': 0.504008948802948, 'start_frame': 268, 'end_frame': 312}, {'action': 3, 'confidence': 0.8814914226531982, 'start_frame': 290, 'end_frame': 334}, {'action': -1, 'confidence': 0.5344507694244385, 'start_frame': 312, 'end_frame': 356}, {'action': 2, 'confidence': 0.8156806826591492, 'start_frame': 334, 'end_frame': 378}, {'action': 2, 'confidence': 0.9804926514625549, 'start_frame': 356, 'end_frame': 400}, {'action': 2, 'confidence': 0.6888275146484375, 'start_frame': 378, 'end_frame': 422}, {'action': 3, 'confidence': 0.9845178723335266, 'start_frame': 400, 'end_frame': 444}, {'action': 3, 'confidence': 0.9667542576789856, 'start_frame': 422, 'end_frame': 466}, {'action': -1, 'confidence': 0.3745054602622986, 'start_frame': 444, 'end_frame': 488}, {'action': 2, 'confidence': 0.6568078398704529, 'start_frame': 466, 'end_frame': 510}, {'action': 2, 'confidence': 0.9288411736488342, 'start_frame': 488, 'end_frame': 532}, {'action': 2, 'confidence': 0.9105760455131531, 'start_frame': 510, 'end_frame': 554}, {'action': -1, 'confidence': 0.5747328996658325, 'start_frame': 532, 'end_frame': 576}], 2: [{'action': 3, 'confidence': 0.9850569367408752, 'start_frame': 18, 'end_frame': 62}, {'action': 3, 'confidence': 0.9942229390144348, 'start_frame': 40, 'end_frame': 92}, {'action': 3, 'confidence': 0.8107689619064331, 'start_frame': 62, 'end_frame': 114}, {'action': 3, 'confidence': 0.7827928066253662, 'start_frame': 92, 'end_frame': 136}, {'action': 0, 'confidence': 0.6350699067115784, 'start_frame': 114, 'end_frame': 158}, {'action': -1, 'confidence': 0.5994275212287903, 'start_frame': 136, 'end_frame': 180}, {'action': 3, 'confidence': 0.9943222999572754, 'start_frame': 158, 'end_frame': 202}, {'action': -1, 'confidence': 0.3747454583644867, 'start_frame': 180, 'end_frame': 224}, {'action': -1, 'confidence': 0.459738165140152, 'start_frame': 202, 'end_frame': 246}, {'action': -1, 'confidence': 0.5235802531242371, 'start_frame': 224, 'end_frame': 268}, {'action': -1, 'confidence': 0.3999873995780945, 'start_frame': 246, 'end_frame': 290}, {'action': -1, 'confidence': 0.5983824133872986, 'start_frame': 268, 'end_frame': 312}, {'action': 3, 'confidence': 0.7354435324668884, 'start_frame': 290, 'end_frame': 334}, {'action': 3, 'confidence': 0.9813284873962402, 'start_frame': 312, 'end_frame': 356}, {'action': 3, 'confidence': 0.6291055679321289, 'start_frame': 334, 'end_frame': 378}, {'action': 3, 'confidence': 0.9783114790916443, 'start_frame': 356, 'end_frame': 400}, {'action': 3, 'confidence': 0.8235399127006531, 'start_frame': 378, 'end_frame': 422}, {'action': 0, 'confidence': 0.7331441044807434, 'start_frame': 400, 'end_frame': 444}, {'action': 2, 'confidence': 0.7732349634170532, 'start_frame': 422, 'end_frame': 466}, {'action': 3, 'confidence': 0.9600069522857666, 'start_frame': 444, 'end_frame': 488}, {'action': 3, 'confidence': 0.9654788970947266, 'start_frame': 466, 'end_frame': 510}, {'action': 3, 'confidence': 0.7804905772209167, 'start_frame': 488, 'end_frame': 532}, {'action': 3, 'confidence': 0.8744498491287231, 'start_frame': 510, 'end_frame': 554}, {'action': -1, 'confidence': 0.46795931458473206, 'start_frame': 532, 'end_frame': 576}, {'action': 2, 'confidence': 0.9364983439445496, 'start_frame': 554, 'end_frame': 598}, {'action': 2, 'confidence': 0.9875214695930481, 'start_frame': 576, 'end_frame': 620}, {'action': -1, 'confidence': 0.5749558806419373, 'start_frame': 598, 'end_frame': 642}, {'action': 1, 'confidence': 0.677196204662323, 'start_frame': 620, 'end_frame': 664}, {'action': -1, 'confidence': 0.4859815835952759, 'start_frame': 642, 'end_frame': 686}, {'action': 2, 'confidence': 0.9400076866149902, 'start_frame': 664, 'end_frame': 708}, {'action': 2, 'confidence': 0.6460714936256409, 'start_frame': 686, 'end_frame': 730}, {'action': 2, 'confidence': 0.7775340676307678, 'start_frame': 708, 'end_frame': 752}, {'action': 3, 'confidence': 0.7818917632102966, 'start_frame': 730, 'end_frame': 774}, {'action': 3, 'confidence': 0.6691318154335022, 'start_frame': 752, 'end_frame': 796}, {'action': -1, 'confidence': 0.5237624049186707, 'start_frame': 774, 'end_frame': 818}, {'action': 2, 'confidence': 0.8456557989120483, 'start_frame': 796, 'end_frame': 840}, {'action': 2, 'confidence': 0.974567711353302, 'start_frame': 818, 'end_frame': 862}, {'action': 2, 'confidence': 0.6545299887657166, 'start_frame': 840, 'end_frame': 884}, {'action': 3, 'confidence': 0.9834510087966919, 'start_frame': 862, 'end_frame': 906}, {'action': 3, 'confidence': 0.9943045973777771, 'start_frame': 884, 'end_frame': 928}, {'action': 3, 'confidence': 0.9878858327865601, 'start_frame': 906, 'end_frame': 950}, {'action': 3, 'confidence': 0.9925846457481384, 'start_frame': 928, 'end_frame': 972}, {'action': 3, 'confidence': 0.9961431622505188, 'start_frame': 950, 'end_frame': 994}, {'action': 3, 'confidence': 0.9568811655044556, 'start_frame': 972, 'end_frame': 1016}, {'action': 3, 'confidence': 0.9245779514312744, 'start_frame': 994, 'end_frame': 1038}, {'action': 3, 'confidence': 0.9065361022949219, 'start_frame': 1016, 'end_frame': 1060}, {'action': 3, 'confidence': 0.9309154748916626, 'start_frame': 1038, 'end_frame': 1082}, {'action': 3, 'confidence': 0.9898011088371277, 'start_frame': 1060, 'end_frame': 1104}, {'action': 3, 'confidence': 0.6210454702377319, 'start_frame': 1082, 'end_frame': 1126}, {'action': 2, 'confidence': 0.9343445301055908, 'start_frame': 1104, 'end_frame': 1148}, {'action': 2, 'confidence': 0.7782953381538391, 'start_frame': 1126, 'end_frame': 1170}, {'action': 2, 'confidence': 0.6740386486053467, 'start_frame': 1148, 'end_frame': 1192}, {'action': 2, 'confidence': 0.857856035232544, 'start_frame': 1170, 'end_frame': 1214}, {'action': 2, 'confidence': 0.8001340627670288, 'start_frame': 1192, 'end_frame': 1236}, {'action': 2, 'confidence': 0.7670595049858093, 'start_frame': 1214, 'end_frame': 1258}, {'action': 2, 'confidence': 0.9792895317077637, 'start_frame': 1236, 'end_frame': 1280}, {'action': 3, 'confidence': 0.9654703736305237, 'start_frame': 1258, 'end_frame': 1302}, {'action': 3, 'confidence': 0.9929152727127075, 'start_frame': 1280, 'end_frame': 1324}, {'action': 3, 'confidence': 0.9799473285675049, 'start_frame': 1302, 'end_frame': 1346}, {'action': 3, 'confidence': 0.9696595072746277, 'start_frame': 1324, 'end_frame': 1368}, {'action': 3, 'confidence': 0.9250435829162598, 'start_frame': 1346, 'end_frame': 1390}, {'action': 3, 'confidence': 0.9499350786209106, 'start_frame': 1368, 'end_frame': 1412}, {'action': 3, 'confidence': 0.6612452864646912, 'start_frame': 1390, 'end_frame': 1434}, {'action': 3, 'confidence': 0.6138421297073364, 'start_frame': 1412, 'end_frame': 1456}, {'action': 3, 'confidence': 0.8024263381958008, 'start_frame': 1434, 'end_frame': 1478}], 3: [{'action': 3, 'confidence': 0.8087773323059082, 'start_frame': 18, 'end_frame': 62}, {'action': 3, 'confidence': 0.9598264694213867, 'start_frame': 40, 'end_frame': 92}, {'action': 0, 'confidence': 0.6841532588005066, 'start_frame': 62, 'end_frame': 114}, {'action': 0, 'confidence': 0.9471869468688965, 'start_frame': 92, 'end_frame': 136}, {'action': 0, 'confidence': 0.9127034544944763, 'start_frame': 114, 'end_frame': 158}, {'action': -1, 'confidence': 0.577185332775116, 'start_frame': 136, 'end_frame': 180}, {'action': 3, 'confidence': 0.986062228679657, 'start_frame': 158, 'end_frame': 202}, {'action': 3, 'confidence': 0.9974262118339539, 'start_frame': 180, 'end_frame': 224}, {'action': 3, 'confidence': 0.9477106928825378, 'start_frame': 202, 'end_frame': 246}, {'action': -1, 'confidence': 0.534584105014801, 'start_frame': 224, 'end_frame': 268}, {'action': 2, 'confidence': 0.7285982966423035, 'start_frame': 246, 'end_frame': 290}, {'action': 3, 'confidence': 0.7207287549972534, 'start_frame': 268, 'end_frame': 312}, {'action': 3, 'confidence': 0.7883838415145874, 'start_frame': 290, 'end_frame': 334}, {'action': -1, 'confidence': 0.38702595233917236, 'start_frame': 312, 'end_frame': 356}, {'action': -1, 'confidence': 0.4189753234386444, 'start_frame': 334, 'end_frame': 378}, {'action': 3, 'confidence': 0.8872455954551697, 'start_frame': 356, 'end_frame': 400}, {'action': 3, 'confidence': 0.9521068930625916, 'start_frame': 378, 'end_frame': 422}, {'action': 3, 'confidence': 0.6139706373214722, 'start_frame': 400, 'end_frame': 444}, {'action': 3, 'confidence': 0.7351241707801819, 'start_frame': 422, 'end_frame': 466}, {'action': -1, 'confidence': 0.5747543573379517, 'start_frame': 444, 'end_frame': 488}, {'action': 0, 'confidence': 0.980269730091095, 'start_frame': 466, 'end_frame': 510}, {'action': 0, 'confidence': 0.901904821395874, 'start_frame': 488, 'end_frame': 532}, {'action': 0, 'confidence': 0.9745803475379944, 'start_frame': 510, 'end_frame': 554}, {'action': 1, 'confidence': 0.7723713517189026, 'start_frame': 532, 'end_frame': 576}, {'action': -1, 'confidence': 0.43138208985328674, 'start_frame': 554, 'end_frame': 598}, {'action': -1, 'confidence': 0.37346798181533813, 'start_frame': 576, 'end_frame': 620}, {'action': -1, 'confidence': 0.4855022430419922, 'start_frame': 598, 'end_frame': 642}, {'action': 3, 'confidence': 0.9364086389541626, 'start_frame': 620, 'end_frame': 664}, {'action': 3, 'confidence': 0.8572442531585693, 'start_frame': 642, 'end_frame': 686}, {'action': 3, 'confidence': 0.9342221021652222, 'start_frame': 664, 'end_frame': 708}, {'action': 3, 'confidence': 0.9939279556274414, 'start_frame': 686, 'end_frame': 730}, {'action': 3, 'confidence': 0.9510948657989502, 'start_frame': 708, 'end_frame': 752}, {'action': 1, 'confidence': 0.9041907787322998, 'start_frame': 730, 'end_frame': 774}, {'action': 2, 'confidence': 0.7395765781402588, 'start_frame': 752, 'end_frame': 796}, {'action': 3, 'confidence': 0.8558378219604492, 'start_frame': 774, 'end_frame': 818}, {'action': -1, 'confidence': 0.5956960916519165, 'start_frame': 796, 'end_frame': 840}, {'action': 3, 'confidence': 0.8807011246681213, 'start_frame': 818, 'end_frame': 862}, {'action': 3, 'confidence': 0.9804959893226624, 'start_frame': 840, 'end_frame': 884}, {'action': 3, 'confidence': 0.651612401008606, 'start_frame': 862, 'end_frame': 906}, {'action': 3, 'confidence': 0.8957451581954956, 'start_frame': 884, 'end_frame': 928}, {'action': 3, 'confidence': 0.9845309257507324, 'start_frame': 906, 'end_frame': 950}, {'action': 3, 'confidence': 0.9639245271682739, 'start_frame': 928, 'end_frame': 972}, {'action': -1, 'confidence': 0.5889495015144348, 'start_frame': 950, 'end_frame': 994}, {'action': -1, 'confidence': 0.4938529431819916, 'start_frame': 972, 'end_frame': 1016}, {'action': 0, 'confidence': 0.8011042475700378, 'start_frame': 994, 'end_frame': 1038}, {'action': 0, 'confidence': 0.8459349870681763, 'start_frame': 1016, 'end_frame': 1060}, {'action': 0, 'confidence': 0.603818416595459, 'start_frame': 1038, 'end_frame': 1082}, {'action': 1, 'confidence': 0.9031785130500793, 'start_frame': 1060, 'end_frame': 1104}, {'action': 1, 'confidence': 0.9297332167625427, 'start_frame': 1082, 'end_frame': 1126}, {'action': 2, 'confidence': 0.6327998042106628, 'start_frame': 1104, 'end_frame': 1148}, {'action': 1, 'confidence': 0.6192783117294312, 'start_frame': 1126, 'end_frame': 1170}, {'action': 1, 'confidence': 0.7230226397514343, 'start_frame': 1148, 'end_frame': 1192}, {'action': 2, 'confidence': 0.8357176780700684, 'start_frame': 1170, 'end_frame': 1214}, {'action': -1, 'confidence': 0.5580663084983826, 'start_frame': 1192, 'end_frame': 1236}, {'action': 2, 'confidence': 0.7812924385070801, 'start_frame': 1214, 'end_frame': 1258}, {'action': 3, 'confidence': 0.9916355609893799, 'start_frame': 1236, 'end_frame': 1280}, {'action': 3, 'confidence': 0.9081491231918335, 'start_frame': 1258, 'end_frame': 1302}, {'action': 3, 'confidence': 0.9536014795303345, 'start_frame': 1280, 'end_frame': 1324}], 18: [{'action': 3, 'confidence': 0.9946274161338806, 'start_frame': 18, 'end_frame': 62}]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
